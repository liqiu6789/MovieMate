# 安装依赖 (运行前取消注释)
# !pip install sentence-transformers langchain faiss-cpu requests langchain_community pymupdf python-docx PyPDF2

import os
import logging
from pathlib import Path
from typing import List
from datetime import datetime
import fitz  # PDF处理库（PyMuPDF）
from docx import Document as DocxDocument  # DOCX处理库
import requests
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores.faiss import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings

# 配置日志系统
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('rag_system.log')
    ]
)
logger = logging.getLogger("RAGSystem")

# 配置参数
LOCAL_MODEL_PATH = r"C:\Users\Administrator\PycharmProjects\MovieMate\sentence-transformers\all-MiniLM-L6-v2"
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEEPSEEK_API_KEY = "sk-e330538ce6644cb5aa04785fdd0c9839"

class RAGSystem:
    def __init__(self, data_dir: str = "./data"):
        logger.info("Initializing RAG System...")
        start_time = datetime.now()

        # 检查本地模型存在性（保持原样）
        if not Path(LOCAL_MODEL_PATH).exists():
            raise FileNotFoundError(
                f"模型路径 {LOCAL_MODEL_PATH} 不存在，请先下载模型\n"
                "运行指令：python -c \"from sentence_transformers import SentenceTransformer as ST;"
                f"ST('sentence-transformers/all-MiniLM-L6-v2').save(r'{LOCAL_MODEL_PATH}')\""
            )

        # 初始化嵌入模型（保持原样）
        logger.info(f"Loading embedding model from: {LOCAL_MODEL_PATH}")
        self.embeddings = HuggingFaceEmbeddings(
            model_name=LOCAL_MODEL_PATH,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': False}
        )
        logger.info("Embedding model loaded successfully")

        # 加载和处理数据（修改文件类型支持）
        logger.info(f"Loading documents from: {data_dir}")
        self.documents = self.load_documents(data_dir)
        logger.info(f"Loaded {len(self.documents)} documents")

        # 以下保持不变...
        logger.info("Splitting documents into chunks")
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.chunks = self.split_documents(self.documents)
        logger.info(f"Split into {len(self.chunks)} chunks")

        logger.info("Building FAISS vector database")
        self.vector_db = FAISS.from_documents(
            documents=self.chunks,
            embedding=self.embeddings
        )
        logger.info(f"VectorDB contains {self.vector_db.index.ntotal} vectors")
        logger.info(f"Initialization completed in {datetime.now() - start_time}")

    def load_documents(self, data_dir: str) -> List[Document]:
        """加载支持多种格式的文档"""
        logger.debug(f"Scanning directory: {data_dir}")
        docs = []
        
        # 遍历所有支持的文件类型
        for ext in ["*.txt", "*.pdf", "*.docx"]:
            for p in Path(data_dir).glob(ext):
                if not p.is_file():
                    continue
                try:
                    logger.debug(f"Processing file: {p.name}")
                    content = self._load_single_document(p)
                    docs.append(Document(
                        page_content=content,
                        metadata={"source": p.name}
                    ))
                except Exception as e:
                    logger.error(f"Error loading {p}: {str(e)}")
        return docs

    def _load_single_document(self, file_path: Path) -> str:
        """根据文件类型选择加载方法"""
        ext = file_path.suffix.lower()
        if ext == ".pdf":
            return self._process_pdf(file_path)
        elif ext == ".docx":
            return self._process_docx(file_path)
        elif ext == ".txt":
            return self._process_txt(file_path)
        else:
            raise ValueError(f"Unsupported file type: {ext}")

    def _process_pdf(self, file_path: Path) -> str:
        """处理PDF文件"""
        text = ""
        try:
            with fitz.open(file_path) as doc:
                for page in doc:
                    text += page.get_text()
            return text
        except Exception as e:
            logger.error(f"PDF解析失败 {file_path}: {str(e)}")
            raise

    def _process_docx(self, file_path: Path) -> str:
        """处理DOCX文件"""
        try:
            doc = DocxDocument(file_path)
            return "\n".join([para.text for para in doc.paragraphs])
        except Exception as e:
            logger.error(f"DOCX解析失败 {file_path}: {str(e)}")
            raise

    def _process_txt(self, file_path: Path) -> str:
        """处理TXT文件"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # 尝试其他编码格式
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    return f.read()
            except Exception as e:
                logger.error(f"TXT解析失败 {file_path}: {str(e)}")
                raise

    # 以下保持不变...
    def split_documents(self, documents: List[Document]) -> List[Document]:
        return self.text_splitter.split_documents(documents)

    def retrieve(self, query: str, k: int = 3) -> List[Document]:
        # 保持原有实现...

    def generate(self, query: str, context: List[Document]) -> str:
        # 保持原有实现...

    def query(self, question: str) -> str:
        # 保持原有实现...

if __name__ == "__main__":
    # 主程序保持不变...